{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxZjOLnwtUyg"
      },
      "source": [
        "# Statistical Techniques for Classifiers Comparison with Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnkS9xxVpEqx"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random as rn\n",
        "import os\n",
        "from sklearn import model_selection\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed for reproducability\n",
        "seed = 1234\n",
        "np.random.seed(seed)\n",
        "rn.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)"
      ],
      "metadata": {
        "id": "twWtwAXuKN2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30swfqoiZQrh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b09ed6e1-ee7c-486d-d15a-86958f049895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "train_data, test_data = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.concatenate((train_data[0], test_data[0]))\n",
        "Y = np.concatenate((train_data[1], test_data[1]))"
      ],
      "metadata": {
        "id": "BLUKAYFUENIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(\n",
        "X, Y, test_size=0.15, random_state=42, stratify = Y)"
      ],
      "metadata": {
        "id": "hfGcU7y0EWAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train val split\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(\n",
        "train_images, train_labels, test_size=0.15, random_state=42, stratify = train_labels)"
      ],
      "metadata": {
        "id": "FiN8-Lv8E5H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq8aDetNuRK5"
      },
      "outputs": [],
      "source": [
        "# Reshape the labels and encode them categorically.\n",
        "y_train = tf.keras.utils.to_categorical(train_labels)\n",
        "y_test = tf.keras.utils.to_categorical(test_labels)\n",
        "y_val =  tf.keras.utils.to_categorical(val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOxJv_26Zu2n"
      },
      "outputs": [],
      "source": [
        "# Reshape and normalize the images.\n",
        "X_train = train_images.reshape((train_images.shape[0], 784))\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = test_images.reshape((test_images.shape[0], 784))\n",
        "X_test = X_test.astype('float32') / 255\n",
        "X_val = val_images.reshape((val_images.shape[0], 784))\n",
        "X_val = X_val.astype('float32') / 255\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sznihRFtr7X"
      },
      "outputs": [],
      "source": [
        "# Define model 1: Fully connected NN\n",
        "\n",
        "nn = tf.keras.models.Sequential()\n",
        "# Add two fully-connected layers to the network.\n",
        "nn.add(tf.keras.layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "nn.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "nn.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
        "\n",
        "# Define the parameters\n",
        "num_epochs = 10\n",
        "batch_size = 256"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model.\n",
        "history = nn.fit(X_train,\n",
        "                      y_train,\n",
        "                      epochs=num_epochs,\n",
        "                      batch_size=batch_size,\n",
        "                      validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSGeLX0pGw8V",
        "outputId": "4b28548a-78e1-4f14-c351-4bbda6aa7806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "198/198 [==============================] - 3s 5ms/step - loss: 0.3501 - accuracy: 0.9022 - val_loss: 0.1898 - val_accuracy: 0.9460\n",
            "Epoch 2/10\n",
            "198/198 [==============================] - 1s 4ms/step - loss: 0.1509 - accuracy: 0.9561 - val_loss: 0.1336 - val_accuracy: 0.9597\n",
            "Epoch 3/10\n",
            "198/198 [==============================] - 1s 4ms/step - loss: 0.1014 - accuracy: 0.9706 - val_loss: 0.1026 - val_accuracy: 0.9692\n",
            "Epoch 4/10\n",
            "198/198 [==============================] - 1s 4ms/step - loss: 0.0732 - accuracy: 0.9794 - val_loss: 0.0908 - val_accuracy: 0.9722\n",
            "Epoch 5/10\n",
            "198/198 [==============================] - 1s 4ms/step - loss: 0.0562 - accuracy: 0.9842 - val_loss: 0.0844 - val_accuracy: 0.9751\n",
            "Epoch 6/10\n",
            "198/198 [==============================] - 1s 4ms/step - loss: 0.0430 - accuracy: 0.9885 - val_loss: 0.0779 - val_accuracy: 0.9756\n",
            "Epoch 7/10\n",
            "198/198 [==============================] - 1s 4ms/step - loss: 0.0334 - accuracy: 0.9914 - val_loss: 0.0735 - val_accuracy: 0.9769\n",
            "Epoch 8/10\n",
            "198/198 [==============================] - 1s 4ms/step - loss: 0.0261 - accuracy: 0.9936 - val_loss: 0.0795 - val_accuracy: 0.9756\n",
            "Epoch 9/10\n",
            "198/198 [==============================] - 1s 4ms/step - loss: 0.0213 - accuracy: 0.9953 - val_loss: 0.0697 - val_accuracy: 0.9773\n",
            "Epoch 10/10\n",
            "198/198 [==============================] - 1s 4ms/step - loss: 0.0163 - accuracy: 0.9967 - val_loss: 0.0713 - val_accuracy: 0.9785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "zZjoPLkNvzCo",
        "outputId": "a8a3868e-dbf9-4dda-d988-daf6ed901daf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(max_depth=30, random_state=1234)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=30, random_state=1234)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=30, random_state=1234)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Define model 2: Single Decision Tree\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# Train baseline decision tree model\n",
        "clf = DecisionTreeClassifier(max_depth=30, random_state=seed)\n",
        "clf.fit(X_train.reshape(-1, 28*28), y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKqdIRBiwe6Q",
        "outputId": "12cac4af-2edb-4160-c836-e2ba17fde19b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "329/329 [==============================] - 1s 2ms/step\n",
            "CLF     0     1     2     3     4    5     6     7    8     9    All\n",
            "NN                                                                  \n",
            "0     975     1    12    12     1   10    13    15    9     3   1051\n",
            "1       0  1140    17     6     2    2     8     9    9     5   1198\n",
            "2      11    14   884    31    14    9    15    22   22     6   1028\n",
            "3      10     7    23   911     6   38     6    17   29    33   1080\n",
            "4       5     2    13     9   890    6    11    21   14    62   1033\n",
            "5      17     6     7    50    10  770    32     6   22    18    938\n",
            "6      12     3    13     9    17   28   917     4   20     7   1030\n",
            "7       3    10    25    13    12    5     1   988    8    32   1097\n",
            "8      11     9    39    27    21   36    16    10  822    32   1023\n",
            "9      11     5    13    18    52   22     3    22   15   861   1022\n",
            "All  1055  1197  1046  1086  1025  926  1022  1114  970  1059  10500\n"
          ]
        }
      ],
      "source": [
        "ct = pd.crosstab(np.argmax(nn.predict(X_test), axis=1), np.argmax(clf.predict(X_test), axis=1), rownames=['NN'], colnames=['CLF'], margins=True)\n",
        "print(ct)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ctab = ct.iloc[:-1,:-1]\n",
        "contingency_table = ctab.copy()\n",
        "print(contingency_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaMprbEvOXsj",
        "outputId": "9c4f315b-0d9a-4a63-baff-d30b10c6e42a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLF    0     1    2    3    4    5    6    7    8    9\n",
            "NN                                                    \n",
            "0    975     1   12   12    1   10   13   15    9    3\n",
            "1      0  1140   17    6    2    2    8    9    9    5\n",
            "2     11    14  884   31   14    9   15   22   22    6\n",
            "3     10     7   23  911    6   38    6   17   29   33\n",
            "4      5     2   13    9  890    6   11   21   14   62\n",
            "5     17     6    7   50   10  770   32    6   22   18\n",
            "6     12     3   13    9   17   28  917    4   20    7\n",
            "7      3    10   25   13   12    5    1  988    8   32\n",
            "8     11     9   39   27   21   36   16   10  822   32\n",
            "9     11     5   13   18   52   22    3   22   15  861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwyDMud15lWJ"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "def apply_sweep(x, MARGIN, FUN=np.subtract, STATS=None, check_margin=True, **kwargs):\n",
        "    '''\n",
        "    A Python version of the R function apply_sweep() from the basic R syntax.\n",
        "    The sweep R function applies an operation (e.g. + or -) to a data matrix by row or by column.\n",
        "\n",
        "\n",
        "    Parameters:\n",
        "    -----------------------\n",
        "    x: Typically a matrix.\n",
        "    MARGIN: Specifies typically whether the operation should be applied by row or by column. MARGIN = 1 operates by row; MARGIN = 2 operates by column.\n",
        "    STATS: Specifies usually the value that should be used for the operation (e.g. the value that should be added or subtracted).\n",
        "    FUN: The operation that should be carried out (e.g. + or -).\n",
        "\n",
        "    Returns:\n",
        "    -----------------------\n",
        "    A matrix with the same dimensions as x with the operation applied by row or by column.\n",
        "    '''\n",
        "\n",
        "    if STATS is None:\n",
        "        STATS = np.array([], dtype=x.dtype)\n",
        "\n",
        "    dims = np.shape(x)\n",
        "\n",
        "    if isinstance(MARGIN, str):\n",
        "        dn = np.array(x).dtype.names\n",
        "\n",
        "        if dn is None:\n",
        "            raise ValueError(\"'x' must have named dimnames\")\n",
        "\n",
        "        MARGIN = np.where(dn == MARGIN)[0][0]\n",
        "\n",
        "        if np.any(np.isnan(MARGIN)):\n",
        "            raise ValueError(\"not all elements of 'MARGIN' are names of dimensions\")\n",
        "\n",
        "    if check_margin:\n",
        "        dimmargin = dims[MARGIN]\n",
        "        dimstats = pd.DataFrame(STATS).shape\n",
        "        lstats = np.size(STATS)\n",
        "\n",
        "        if lstats > np.prod(dimmargin):\n",
        "            print(\"Warning: STATS is longer than the extent of 'dim(x)[MARGIN]'\")\n",
        "        elif len(dimstats) == 0:\n",
        "            cumDim = np.append(1, np.cumprod(dimmargin))\n",
        "            upper = np.min(cumDim[cumDim >= lstats])\n",
        "            lower = np.max(cumDim[cumDim <= lstats])\n",
        "\n",
        "            if lstats and (upper % lstats != 0 or lstats % lower != 0):\n",
        "                print(\"Warning: STATS does not recycle exactly across MARGIN\")\n",
        "        else:\n",
        "            dimmargin = np.array(dimmargin).reshape(-1)\n",
        "            dimstats = np.array(dimstats)[:-1]\n",
        "\n",
        "            if len(dimstats) != len(dimmargin) or not np.any(dimstats == dimmargin):\n",
        "                print(\"Warning: length(STATS) or dim(STATS) do not match dim(x)[MARGIN]\")\n",
        "\n",
        "    perm = np.concatenate((np.arange(MARGIN), np.arange(MARGIN+1, len(dims)), [MARGIN]))\n",
        "    return FUN(x, STATS).transpose(perm).T\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# %%\n",
        "\n",
        "# KOHEN'S KAPPA\n",
        "\n",
        "def kappa(x, weights=['Equal-Spacing', 'Fleiss-Cohen']):\n",
        "    '''\n",
        "    Computes two agreement rates: Cohen's kappa and weighted kappa, and confidence bands.\n",
        "    A Python version of the R function Kappa() from the vcd package.\n",
        "\n",
        "    Parameters:\n",
        "    -----------------------\n",
        "    x: array_like or pandas.DataFrame\n",
        "        If `x` is a matrix, it should be a KxK matrix of counts of\n",
        "        assignments of objects to K categories.\n",
        "    weights:veither one of the character strings given in the default value, or a user-specified matrix with same dimensions as x.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "    -----------------------\n",
        "    A dictionary containing the following:\n",
        "    - 'Unweighted': numeric vector of length 2 with the kappa statistic (value component), along with Approximate Standard Error (ASE component).\n",
        "    - 'Weighted': idem for the weighted kappa.\n",
        "    - 'CI': matrix with lower and upper bounds of the 95% confidence interval for the unweighted and weighted kappa.\n",
        "\n",
        "    '''\n",
        "    if isinstance(weights, str):\n",
        "        weights = [weights]\n",
        "    d = np.diag(x)\n",
        "    n = np.sum(x).sum()\n",
        "    nc = x.shape[1]\n",
        "    colFreqs = np.sum(x, axis=0)/n\n",
        "    rowFreqs = np.sum(x, axis=1)/n\n",
        "\n",
        "    def kappa_val(po, pc):\n",
        "        return (po - pc)/(1 - pc)\n",
        "    def std(p, pc, kw, W=None):\n",
        "        dot1 = np.dot(W, np.sum(p, axis=0))\n",
        "        dot2 = np.dot(W, np.sum(p, axis=1) * (1 - kw))\n",
        "        inner1 = np.inner(1, 1-pc)\n",
        "        inner2 = np.inner(1-pc, 1)\n",
        "        sweep1 = apply_sweep( W, MARGIN = 1, STATS = dot1 * (1 - kw))\n",
        "        return  np.sqrt(((np.sum(p * apply_sweep(sweep1, MARGIN=0, STATS = dot2)**2)).sum()- (kw - pc * (1 - kw)) **2) /(inner1 * inner2)/n)\n",
        "\n",
        "    # unweighted kappa\n",
        "    po = np.sum(d)/n\n",
        "    pc = np.inner(colFreqs, rowFreqs)\n",
        "    k = kappa_val(po, pc)\n",
        "    W0 = np.diag(np.ones(nc))\n",
        "    s = std(x/n, pc, k, W0)\n",
        "\n",
        "    # weighted kappa\n",
        "    if isinstance(weights, list):\n",
        "        if weights[0] == 'Equal-Spacing':\n",
        "            W = 1 - np.abs(np.subtract.outer(np.arange(1, nc+1), np.arange(1, nc+1)))/(nc-1)\n",
        "        else:\n",
        "            W = 1 - np.power(np.abs(np.subtract.outer(np.arange(1, nc+1), np.arange(1, nc+1)))/(nc-1), 2)\n",
        "    else:\n",
        "        W = weights\n",
        "\n",
        "    pow = np.sum(W * x).sum()/n\n",
        "    pcw = np.sum(W * np.outer(colFreqs, rowFreqs))\n",
        "    kw = kappa_val(pow, pcw)\n",
        "    sw = std(x/n, pcw, kw, W)\n",
        "\n",
        "    #compute z-score and p-value\n",
        "    # tab <- rbind(x$Unweighted, x$Weighted)\n",
        "    tab = np.vstack((np.array([k, s]), np.array([kw, sw])))\n",
        "    z = tab[:,0] / tab[:,1]\n",
        "    # tab <- cbind(tab, z, `Pr(>|z|)` = 2 * pnorm(-abs(z)))\n",
        "    tab = np.c_[tab, z, 2 * norm.cdf(-abs(z))]\n",
        "\n",
        "    CI = True\n",
        "    if CI:\n",
        "        level = 0.95\n",
        "        q = norm.ppf((1 + level)/2)\n",
        "        lower = tab[:,1] - q * tab[:,2]\n",
        "        upper = tab[:,1] + q * tab[:,2]\n",
        "        tab_ci = np.c_[tab, lower, upper]\n",
        "\n",
        "\n",
        "    return {'Unweighted': {'kappa': k, 'std': s, 'z': z[0], 'p': tab[0,3]},\n",
        "            'Weighted': {'kappa': kw, 'std': sw, 'z': z[1], 'p': tab[1,3]},\n",
        "            'CI': {'lower': lower, 'upper': upper}}\n",
        "\n",
        "\n",
        "# STUART-MAXWELL TEST\n",
        "\n",
        "def StuartMaxwellTest(x, y=None, BhapkarTest=False):\n",
        "    \"\"\"\n",
        "    Stuart-Maxwell test for marginal homogeneity of a KxK matrix of assignments\n",
        "    of objects to K categories or an nx2 or 2xn matrix of category scores for n\n",
        "    data objects by two raters. The statistic is distributed as Chi-square with\n",
        "    K-1 degrees of freedom.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    x : array_like or pandas.DataFrame\n",
        "        If `x` is a matrix, it should be a KxK matrix of counts of\n",
        "        assignments of objects to K categories.\n",
        "    y : array_like, optional\n",
        "        If `x` is a data frame, `y` should not be provided. Otherwise,\n",
        "        `y` should be a KxK matrix with the same counts as `x`.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    result : dict\n",
        "        A dictionary containing the following:\n",
        "        - 'method': str, the name of the test\n",
        "        - 'data.name': str, the name of the data\n",
        "        - 'statistic': float, the test statistic\n",
        "        - 'parameter': int, the degrees of freedom\n",
        "        - 'p.value': float, the p-value of the test\n",
        "        - 'N': int, the total number of observations\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(x, np.ndarray or pd.DataFrame):\n",
        "        r = x.shape[0]\n",
        "        if r < 2 or x.shape[1] != r:\n",
        "            raise ValueError(\"'x' must be square with at least two rows and columns\")\n",
        "        if (x < 0).any().any() or np.isnan(x).any().any():\n",
        "            raise ValueError(\"all entries of 'x' must be nonnegative and finite\")\n",
        "        DNAME = \"x\"\n",
        "    else:\n",
        "        if y is None:\n",
        "            raise ValueError(\"if 'x' is not a matrix, 'y' must be given\")\n",
        "        if y is not None and not isinstance(y, np.ndarray or pd.DataFrame):\n",
        "            raise TypeError(\"'y' must be a matrix\")\n",
        "        if len(x) != len(y):\n",
        "            raise ValueError(\"'x' and 'y' must have the same length\")\n",
        "        DNAME = \" and \".join([str(x), str(y)])\n",
        "        OK = np.logical_and(~np.isnan(x), ~np.isnan(y))\n",
        "        x = np.asarray(x, dtype=np.int64)[OK]\n",
        "        y = np.asarray(y, dtype=np.int64)[OK]\n",
        "        r = np.unique(x).shape[0]\n",
        "        if r < 2 or np.unique(y).shape[0] != r:\n",
        "            raise ValueError(\"'x' and 'y' must have the same number of levels (minimum 2)\")\n",
        "        x = np.asarray(np.histogram2d(x, y, bins=r)[0], dtype=np.int64)\n",
        "\n",
        "    # get the marginals\n",
        "    rowsums = x.sum(axis=1)\n",
        "    colsums = x.sum(axis=0)\n",
        "    #  If you have perfect agreement then you want something along the lines of:\n",
        "    equalsums_rows = np.diag(x) == rowsums\n",
        "    equalsums_cols = np.diag(x) == colsums\n",
        "\n",
        "    equalsums = np.logical_and(equalsums_rows, equalsums_cols)\n",
        "\n",
        "    if equalsums.any():\n",
        "        # dump any categories with perfect agreement\n",
        "        x = x[~equalsums][:, ~equalsums]\n",
        "        # bail out if too many categories have disappeared\n",
        "        if x.shape[0] < 2:\n",
        "            raise ValueError(\"Too many equal marginals, cannot compute\")\n",
        "        # get new marginals\n",
        "        rowsums = x.sum(axis=1)\n",
        "        colsums = x.sum(axis=0)\n",
        "\n",
        "    # use K-1 marginals\n",
        "    Kminus1 = rowsums.shape[0] - 1\n",
        "    smd = rowsums[:-1] - colsums[:-1]\n",
        "    smS = np.zeros((Kminus1, Kminus1))\n",
        "    if isinstance(x, np.ndarray):\n",
        "        x = pd.DataFrame(x)\n",
        "    for i in range(Kminus1):\n",
        "        for j in range(Kminus1):\n",
        "            if i == j:\n",
        "                smS[i,j] = rowsums[i] + colsums[j] - 2 * x.iloc[i,i]\n",
        "            else:\n",
        "                smS[i,j] = -(x.iloc[i,j] + x.iloc[j,i])\n",
        "\n",
        "    N = rowsums.sum()\n",
        "\n",
        "    if BhapkarTest == False:\n",
        "        STATISTIC = smd @ np.linalg.inv(smS) @ smd\n",
        "        PARAMETER = r - 1\n",
        "        METHOD = \"Stuart-Maxwell test\"\n",
        "        # PVAL = stats.chi2.sf(STATISTIC, PARAMETER)\n",
        "        PVAL = stats.chi2.sf(STATISTIC, PARAMETER)\n",
        "\n",
        "    elif BhapkarTest == True:\n",
        "        STATISTIC_pre = smd @ np.linalg.inv(smS) @ smd\n",
        "        STATISTIC = STATISTIC_pre / ( 1- STATISTIC_pre / N)\n",
        "        PARAMETER = r - 1\n",
        "        METHOD = \"Bhapkar test\"\n",
        "        PVAL = stats.chi2.sf(STATISTIC, PARAMETER)\n",
        "\n",
        "\n",
        "    RVAL = {\"statistic\": STATISTIC, \"parameter/dof\": PARAMETER,\n",
        "            \"p.value\": PVAL, \"method\": METHOD, \"data.name\": DNAME,\n",
        "            \"N\": N}\n",
        "    return RVAL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEAKXS9r5hro",
        "outputId": "d32f66b4-aa18-407a-917f-ee893f3ca08b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'kappa': 0.8579282537580654, 'std': 0.003619988151829655, 'z': 236.99753086883493, 'p': 0.0}\n",
            "{'kappa': 0.8615190680780784, 'std': 0.004136479190794951, 'z': 208.27351676160885, 'p': 0.0}\n"
          ]
        }
      ],
      "source": [
        "# Compute Kohen's K\n",
        "KAPPA = kappa(contingency_table)\n",
        "print(KAPPA['Unweighted'])\n",
        "print(KAPPA['Weighted'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STTLM_fi_dsm",
        "outputId": "8803f806-cc44-4cd4-c5ca-af5271f04d17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'statistic': 14.48056578013399, 'parameter/dof': 9, 'p.value': 0.10622809701747711, 'method': 'Stuart-Maxwell test', 'data.name': 'x', 'N': 10500}\n",
            "{'statistic': 14.500563529089417, 'parameter/dof': 9, 'p.value': 0.10560016202044105, 'method': 'Bhapkar test', 'data.name': 'x', 'N': 10500}\n"
          ]
        }
      ],
      "source": [
        "contingency_table = contingency_table.to_numpy()\n",
        "# Stuart-Maxwell test\n",
        "SM = StuartMaxwellTest(contingency_table, BhapkarTest=False)\n",
        "print(SM)\n",
        "# Bhapkar test\n",
        "BP = StuartMaxwellTest(contingency_table, BhapkarTest=True)\n",
        "print(BP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsqOK2t4AOgU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}